---
title: "Week 1"
author: "Me"
date: "12/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(kernlab)
data(spam)
head(spam)
str(spam[,1:5])
dim(spam)
```
## Subsampling our dataset
```{r}
set.seed(3435)
train_indicator = rbinom(4601, size = 1, prob = .5)
table(train_indicator)
```
Divide the set
```{r}
train_spam = spam[train_indicator==1, ]
test_spam = spam[train_indicator==0, ]
```

EDA
- look at summaries
- check for missing data
- create exploratory plots
- perform exploratory analyses (clustering)

```{r}
names(train_spam)
```

```{r}
table(train_spam$type)
```
Plots to compare frequency of certain words
```{r}
plot(log10(train_spam$capitalAve + 1) ~ train_spam$type)
```
Relationship between predictors
```{r}
plot(log10(train_spam[,1:4] + 1))
```
Clustering
```{r}
hcluster = hclust(dist(t(train_spam[,1:57])))
plot(hcluster)
```

Separates out one variable. 

New clustering using transformation
```{r}
hclusterUpdated = hclust(dist(t(log10(train_spam[,1:55]+1))))
plot(hclusterUpdated)
```

## Statistical predictor/modeling
- should be informed by the results of EDA
- exact methods depend on question of interest
- transfomrations/processing should be accounted when necessary
- measures of uncertainty should be reported

Try fit a glm model
```{r}
train_spam$numType = as.numeric(train_spam$type) - 1

cost_function = function(x,y) sum(x != (y > 0.5))
cv_error = rep(NA, 55)

library(boot)
for(i in 1:55){
      lm_formula = reformulate(names(train_spam)[i], response = 'numType')
      glm_fit = glm(lm_formula, family = 'binomial', data = train_spam)
      cv_error[i] = cv.glm(train_spam, glm_fit, cost_function, 2)$delta[2]
}

#predictor has minimum cross validated error.
names(train_spam)[which.min(cv_error)]
```

Get a measure of uncertainty

```{r}
#use best model from the group
prediction_model = glm(numType ~ charDollar, family = 'binomial', data = train_spam)

#get predictions on test set
prediction_test = predict(prediction_model, test_spam)
predicted_spam = rep('nonspam', dim(test_spam)[1])

#classify as spam those with prob > .5
predicted_spam[prediction_model$fitted > .5] = 'spam'
```

```{r}
#classification table

table(predicted_spam, test_spam$type)

##error rate
(61 +458)/(1346 + 458 + 61 + 449)
```

## Interpret results
- use appropriate language
      - describes
      - correlate
      - leads to
      - predicts
- give an explanation
- interpret coefficients
- interpret measures of uncertainty. 

## Example
- fraction of characters that are dollar sign can be used to predict if an email is spam
- anything with more than 6.6% dollar sign is spam
- more dollar means more spam
- our test set error rate was 22.4%

## Challenge results
- Challenge all steps:
      - question
      - data source
      - processing
      - analysis
      - conclusion
- challenge measures of uncertainty
- challenge choice of models
- think of potential alternative analysis.

## write up results
- lead with a question
- summarize analysis into story
- dont include every analysis, include it
      - needed for story
      - needed to address challenge
- order analysis according to story rather than chronologically
- include 'pretty' figures that contribute to the story. 


## Example
- Question
      - can we use quantitative chars of email to classify them as SPAM
- describe the approach
      - collected data from UCI -> created test set and training set
      - explored relationships
      - chose logistics model
      - applied to test 78% accuracy
- interpret results
      - number of dollar signs seem reasonable
- challenge results
      - 78% is not good
      - I could use more variables
      - why logistic regression?


# Organizing a data analysis
Data analysis files
- Data
      - raw data
      - processed data
- Figures
      - exploratory figures
      - final figures
- R code
      - Raw/ unused scripts
      - Final scripts
      - R markdown files
- Text
      - readMe files
      - text of analysis/ Report


Raw data
- if accessed from the web, include URL, description, data accessed in readME

Processed data
- should be named so it is easy to see
- processign script should occur in readMe
- processed data should be tidy

Exploratory figures
- do not need to be pretty
- but usable

Final figures
- small subset of original figures
- axes/colors to make figure clear
- possibly multiple panels

Raw scripts
- less commented
- multiple versions
- may include analyses that are discarded 

Final scripts
- clearly commented
- processing details
- only analyses that appear in final writeup


Rmarkdown files
- text and code together
- very easy to create in Rstudio

Readme files
- should contain instruction for analysis

Text of document
- title, introduction, methods, results, conclusions
- should tell a story
- references should be included

